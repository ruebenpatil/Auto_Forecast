{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c0b73d4-5d2f-47b4-8b47-11a5807907d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pytorch_forecasting'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorch_lightning\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpl\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorch_forecasting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TemporalFusionTransformer\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorch_forecasting\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TimeSeriesDataSet\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorch_forecasting\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SMAPE\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pytorch_forecasting'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso, LinearRegression\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import SMAPE\n",
    "import optuna\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from prophet import Prophet\n",
    "from scipy.stats import skew\n",
    "from itertools import combinations\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"D:\\Projects\\time_series\\Auto_Forecast\\data\\vigorous_daily_90day_data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Try parsing daily date formats\n",
    "parsed = False\n",
    "formats_to_try = [\"%Y-%m-%d\", \"%d-%m-%Y\", \"%m/%d/%Y\"]\n",
    "\n",
    "for fmt in formats_to_try:\n",
    "    try:\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"], format=fmt)\n",
    "        parsed = True\n",
    "        break\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "if not parsed:\n",
    "    try:\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])  # fallback auto-detection\n",
    "        parsed = True\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Unable to parse the 'Date' column. Please use consistent date format.\")\n",
    "        raise e\n",
    "\n",
    "# Set index\n",
    "df.set_index(\"Date\", inplace=True)\n",
    "print(\"‚úÖ Date column successfully parsed and set as index.\")\n",
    "\n",
    "# Identify external features\n",
    "target_column = \"Target\"\n",
    "external_features = [col for col in df.columns if col != target_column]\n",
    "df[external_features] = df[external_features].fillna(0)\n",
    "\n",
    "# Ensure minimum 3 Month of daily data (~90 rows)\n",
    "if len(df) < 90:\n",
    "    print(\"‚ùå Insufficient data. Please provide at least 3 Months (~90 days) of data.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Handle missing target values\n",
    "if df[target_column].isnull().sum() > 0:\n",
    "    print(\"‚ö†Ô∏è Missing values detected. Applying forward fill (ffill).\")\n",
    "    df[target_column].fillna(method='ffill', inplace=True)\n",
    "    if df[target_column].isnull().sum() > 0:\n",
    "        print(\"‚ö†Ô∏è Some values are still missing. Applying linear interpolation.\")\n",
    "        df[target_column].interpolate(method='linear', inplace=True)\n",
    "    print(\"‚ö†Ô∏è Forecast accuracy may be affected due to existence of missing values.\")\n",
    "\n",
    "# Skewness\n",
    "skewness = skew(df[target_column])\n",
    "print(f\"Skewness: {skewness}\")\n",
    "if abs(skewness) > 1:\n",
    "    print(\"‚ö†Ô∏è High skewness detected. Applying log transformation.\")\n",
    "    df[target_column] = np.log(df[target_column] + 1)\n",
    "else:\n",
    "    print(\"‚úÖ Skewness is acceptable. No transformation applied.\")\n",
    "\n",
    "# Outlier detection using IQR\n",
    "Q1, Q3 = df[target_column].quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "outliers = df[(df[target_column] < lower_bound) | (df[target_column] > upper_bound)]\n",
    "if not outliers.empty:\n",
    "    print(\"‚ö†Ô∏è Outliers detected. Replacing with median.\")\n",
    "    df.loc[df[target_column] < lower_bound, target_column] = df[target_column].median()\n",
    "    df.loc[df[target_column] > upper_bound, target_column] = df[target_column].median()\n",
    "\n",
    "# Lag features (daily)\n",
    "for lag in range(1, 31):  # 30 daily lags\n",
    "    df[f'lag_{lag}'] = df[target_column].shift(lag)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# ACF/PACF-based selection\n",
    "acf_values = acf(df[target_column], nlags=30)[1:]\n",
    "pacf_values = pacf(df[target_column], nlags=30)[1:]\n",
    "selected_lags = [i + 1 for i, (a, p) in enumerate(zip(acf_values, pacf_values)) if abs(a) > 0.2 or abs(p) > 0.2]\n",
    "\n",
    "if not selected_lags:\n",
    "    selected_lags = list(range(1, 31))\n",
    "\n",
    "# Remove multicollinear lags via VIF\n",
    "selected_features = [f'lag_{lag}' for lag in selected_lags]\n",
    "while len(selected_features) > 1:\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['Feature'] = selected_features\n",
    "    vif_data['VIF'] = [variance_inflation_factor(df[selected_features].values, i) for i in range(len(selected_features))]\n",
    "    \n",
    "    max_vif = vif_data['VIF'].max()\n",
    "    if max_vif > 5:\n",
    "        selected_features.remove(vif_data.loc[vif_data['VIF'].idxmax(), 'Feature'])\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# RMSE-based lag selection\n",
    "def select_best_lags(train_features, test_features, train_target, test_target, model, max_lags=5):\n",
    "    best_rmse, best_lags = float(\"inf\"), None\n",
    "    all_lags = [col for col in train_features.columns if col.startswith(\"lag_\")]\n",
    "    \n",
    "    for r in range(1, min(len(all_lags), max_lags) + 1):\n",
    "        for lag_subset in combinations(all_lags, r):\n",
    "            try:\n",
    "                train_subset = train_features[list(lag_subset)]\n",
    "                test_subset = test_features[list(lag_subset)]\n",
    "                model.fit(train_subset, train_target)\n",
    "                predictions = model.predict(test_subset)\n",
    "                rmse = np.sqrt(mean_squared_error(test_target, predictions))\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse, best_lags = rmse, list(lag_subset)\n",
    "            except Exception:\n",
    "                continue\n",
    "    \n",
    "    return best_lags if best_lags else all_lags\n",
    "\n",
    "# Prepare train/test split\n",
    "split_index = int(len(df) * 0.8)\n",
    "train_df, test_df = df.iloc[:split_index], df.iloc[split_index:]\n",
    "train_features = train_df[selected_features]\n",
    "test_features = test_df[selected_features]\n",
    "train_target = train_df[target_column]\n",
    "test_target = test_df[target_column]\n",
    "\n",
    "model = LinearRegression()\n",
    "best_lags = select_best_lags(train_features, test_features, train_target, test_target, model)\n",
    "selected_features = best_lags\n",
    "\n",
    "# Final selection via Lasso\n",
    "X = df[selected_features]\n",
    "y = df[target_column]\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "selected_features = [f for f, c in zip(selected_features, lasso.coef_) if c != 0]\n",
    "\n",
    "# Apply rolling statistics dynamically with RMSE optimization (Daily Version)\n",
    "best_window = 3  # Default to 3 days\n",
    "best_rmse = float(\"inf\")\n",
    "train_size = int(len(df) * 0.8)\n",
    "train, test = df.iloc[:train_size], df.iloc[train_size:]\n",
    "\n",
    "for window in range(3, 37):  # Test rolling windows from 3 to 36 days\n",
    "    rolling_mean = train[target_column].rolling(window=window, min_periods=1).mean()\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_column], rolling_mean[-len(test):]))\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_window = window\n",
    "\n",
    "print(f\"‚úÖ Best rolling window found (Daily): {best_window} days\")\n",
    "df[target_column] = df[target_column].rolling(window=best_window, min_periods=1).mean()\n",
    "\n",
    "# Train-test split (re-split after rolling)\n",
    "train_size = int(len(df) * 0.8)\n",
    "train, test = df.iloc[:train_size], df.iloc[train_size:]\n",
    "train_features, test_features = train[selected_features], test[selected_features]\n",
    "train_target, test_target = train[target_column], test[target_column]\n",
    "\n",
    "# Evaluation metrics remain the same\n",
    "def smape(actual, forecast):\n",
    "    return 100 * np.mean(2 * np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast)))\n",
    "\n",
    "def mean_forecast_bias(actual, forecast):\n",
    "    return np.mean(forecast - actual)\n",
    "\n",
    "def evaluate_model(actual, forecast):\n",
    "    return smape(actual, forecast), mean_forecast_bias(actual, forecast)\n",
    "\n",
    "# TFT model with Optuna remains the same\n",
    "def run_tft(train_features, test_features, train_target, test_target):\n",
    "    def objective(trial):\n",
    "        units = trial.suggest_int('units', 32, 256)\n",
    "        dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "        \n",
    "        model = Sequential([\n",
    "            Dense(units, activation='relu', input_shape=(train_features.shape[1],)),\n",
    "            Dropout(dropout),\n",
    "            Dense(units // 2, activation='relu'),\n",
    "            Dropout(dropout),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(train_features, train_target, epochs=50, batch_size=16, verbose=0)\n",
    "        preds = model.predict(test_features).flatten()\n",
    "        return smape(test_target, preds)\n",
    "    \n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=20)\n",
    "    \n",
    "    best_params = study.best_params\n",
    "    model = Sequential([\n",
    "        Dense(best_params['units'], activation='relu', input_shape=(train_features.shape[1],)),\n",
    "        Dropout(best_params['dropout']),\n",
    "        Dense(best_params['units'] // 2, activation='relu'),\n",
    "        Dropout(best_params['dropout']),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(train_features, train_target, epochs=50, batch_size=16, verbose=0)\n",
    "    optimized_preds = model.predict(test_features).flatten()\n",
    "    \n",
    "    baseline_model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(train_features.shape[1],)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    baseline_model.compile(optimizer='adam', loss='mse')\n",
    "    baseline_model.fit(train_features, train_target, epochs=50, batch_size=16, verbose=0)\n",
    "    baseline_preds = baseline_model.predict(test_features).flatten()\n",
    "    \n",
    "    smape_base, mfb_base = evaluate_model(test_target, baseline_preds)\n",
    "    smape_opt, mfb_opt = evaluate_model(test_target, optimized_preds)\n",
    "    \n",
    "    return optimized_preds if smape_opt < smape_base and abs(mfb_opt) < abs(mfb_base) else baseline_preds\n",
    "\n",
    "# LSTM daily version\n",
    "def run_lstm(train_features, test_features, train_target, test_target):\n",
    "    train_X = np.asarray(train_features).reshape(train_features.shape[0], train_features.shape[1], 1)\n",
    "    test_X = np.asarray(test_features).reshape(test_features.shape[0], test_features.shape[1], 1)\n",
    "    \n",
    "    train_y = np.asarray(train_target).reshape(-1, 1)\n",
    "    test_y = np.asarray(test_target).reshape(-1, 1)\n",
    "\n",
    "    def objective(trial):\n",
    "        n_units = trial.suggest_int(\"n_units\", 16, 128)\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "        learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "\n",
    "        model = Sequential([\n",
    "            LSTM(n_units, activation='relu', return_sequences=True, input_shape=(train_X.shape[1], 1)),\n",
    "            Dropout(dropout_rate),\n",
    "            LSTM(n_units, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "        model.fit(train_X, train_y, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "        predictions = model.predict(test_X).flatten()\n",
    "        return smape(test_y, predictions)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_model = Sequential([\n",
    "        LSTM(best_trial.params[\"n_units\"], activation='relu', return_sequences=True, input_shape=(train_X.shape[1], 1)),\n",
    "        Dropout(best_trial.params[\"dropout_rate\"]),\n",
    "        LSTM(best_trial.params[\"n_units\"], activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    best_model.compile(optimizer=Adam(learning_rate=best_trial.params[\"learning_rate\"]), loss='mse')\n",
    "    best_model.fit(train_X, train_y, epochs=50, batch_size=16, verbose=0)\n",
    "    final_predictions = best_model.predict(test_X).flatten()\n",
    "    \n",
    "    return final_predictions\n",
    "\n",
    "# GRU daily version\n",
    "def run_gru(train_features, test_features, train_target, test_target):\n",
    "    train_X = np.asarray(train_features).reshape(train_features.shape[0], train_features.shape[1], 1)\n",
    "    test_X = np.asarray(test_features).reshape(test_features.shape[0], test_features.shape[1], 1)\n",
    "\n",
    "    train_y = np.asarray(train_target).reshape(-1, 1)\n",
    "    test_y = np.asarray(test_target).reshape(-1, 1)\n",
    "\n",
    "    def objective(trial):\n",
    "        n_units = trial.suggest_int(\"n_units\", 16, 128)\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "        learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "\n",
    "        model = Sequential([\n",
    "            GRU(n_units, activation='relu', return_sequences=True, input_shape=(train_X.shape[1], 1)),\n",
    "            Dropout(dropout_rate),\n",
    "            GRU(n_units, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "        model.fit(train_X, train_y, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "        predictions = model.predict(test_X).flatten()\n",
    "        return smape(test_y, predictions)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_model = Sequential([\n",
    "        GRU(best_trial.params[\"n_units\"], activation='relu', return_sequences=True, input_shape=(train_X.shape[1], 1)),\n",
    "        Dropout(best_trial.params[\"dropout_rate\"]),\n",
    "        GRU(best_trial.params[\"n_units\"], activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    best_model.compile(optimizer=Adam(learning_rate=best_trial.params[\"learning_rate\"]), loss='mse')\n",
    "    best_model.fit(train_X, train_y, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "    baseline_model = Sequential([\n",
    "        GRU(50, activation='relu', return_sequences=True, input_shape=(train_X.shape[1], 1)),\n",
    "        GRU(50, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    baseline_model.compile(optimizer='adam', loss='mse')\n",
    "    baseline_model.fit(train_X, train_y, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "    best_preds = best_model.predict(test_X).flatten()\n",
    "    baseline_preds = baseline_model.predict(test_X).flatten()\n",
    "\n",
    "    if smape(test_y, best_preds) < smape(test_y, baseline_preds) and abs(mean_forecast_bias(test_y, best_preds)) < abs(mean_forecast_bias(test_y, baseline_preds)):\n",
    "        return best_preds\n",
    "    else:\n",
    "        return baseline_preds\n",
    "\n",
    "# --- Metrics ---\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_pred) + np.abs(y_true)))\n",
    "\n",
    "def mfb(y_true, y_pred):\n",
    "    return 100 * np.mean((y_pred - y_true) / y_true)\n",
    "\n",
    "def mean_forecast_bias(actual, forecast):\n",
    "    return np.mean(actual - forecast)\n",
    "\n",
    "# --- ARIMA (Daily) ---\n",
    "def run_arima(train, test):\n",
    "    best_loss, best_order = float(\"inf\"), None\n",
    "    lambda_mfb = 0.5\n",
    "    \n",
    "    for p in range(3):\n",
    "        for d in range(2):\n",
    "            for q in range(3):\n",
    "                try:\n",
    "                    model = ARIMA(train, order=(p, d, q)).fit()\n",
    "                    pred = model.forecast(steps=len(test))\n",
    "                    loss = smape(test, pred) + lambda_mfb * abs(mfb(test, pred))\n",
    "                    if loss < best_loss:\n",
    "                        best_loss, best_order = loss, (p, d, q)\n",
    "                except:\n",
    "                    continue\n",
    "                    \n",
    "    final_model = ARIMA(train, order=best_order).fit()\n",
    "    return final_model.forecast(steps=len(test))\n",
    "\n",
    "# --- SARIMA (Daily) ---\n",
    "def run_sarima(train, test):\n",
    "    best_score, best_order, best_seasonal_order = float(\"inf\"), None, None\n",
    "    for p in range(3):\n",
    "        for d in range(2):\n",
    "            for q in range(3):\n",
    "                for P in range(2):\n",
    "                    for D in range(2):\n",
    "                        for Q in range(2):\n",
    "                            try:\n",
    "                                model = SARIMAX(train, order=(p, d, q), seasonal_order=(P, D, Q, 7)).fit()\n",
    "                                pred = model.forecast(steps=len(test))\n",
    "                                combined_score = smape(test, pred) + abs(mean_forecast_bias(test, pred))\n",
    "                                if combined_score < best_score:\n",
    "                                    best_score, best_order, best_seasonal_order = combined_score, (p, d, q), (P, D, Q, 7)\n",
    "                            except:\n",
    "                                continue\n",
    "    return SARIMAX(train, order=best_order, seasonal_order=best_seasonal_order).fit().forecast(steps=len(test))\n",
    "\n",
    "# --- ETS (Daily with Optuna) ---\n",
    "def run_ets(train, test):\n",
    "    def objective(trial):\n",
    "        trend = trial.suggest_categorical(\"trend\", [\"add\", \"mul\", None])\n",
    "        seasonal = trial.suggest_categorical(\"seasonal\", [\"add\", \"mul\", None])\n",
    "        seasonal_periods = trial.suggest_int(\"seasonal_periods\", 2, min(30, len(train) // 2))\n",
    "\n",
    "        try:\n",
    "            model = ExponentialSmoothing(train, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods)\n",
    "            preds = model.fit().forecast(len(test))\n",
    "            return smape(test, preds)\n",
    "        except:\n",
    "            return float(\"inf\")\n",
    "        \n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    optimized_model = ExponentialSmoothing(train, trend=best_params[\"trend\"], seasonal=best_params[\"seasonal\"], \n",
    "                                           seasonal_periods=best_params[\"seasonal_periods\"]).fit()\n",
    "    optimized_preds = optimized_model.forecast(len(test))\n",
    "\n",
    "    baseline_model = ExponentialSmoothing(train, trend=\"add\", seasonal=\"add\", seasonal_periods=min(7, len(train) // 2)).fit()\n",
    "    baseline_preds = baseline_model.forecast(len(test))\n",
    "\n",
    "    return optimized_preds if smape(test, optimized_preds) < smape(test, baseline_preds) else baseline_preds\n",
    "\n",
    "# --- Holt-Winters (Daily) ---\n",
    "def run_holt_winters(train, test):\n",
    "    train_series = pd.Series(train)\n",
    "\n",
    "    def objective(trial):\n",
    "        trend = trial.suggest_categorical(\"trend\", [\"add\", \"mul\", None])\n",
    "        seasonal = trial.suggest_categorical(\"seasonal\", [\"add\", \"mul\", None])\n",
    "        seasonal_period = trial.suggest_int(\"seasonal_periods\", 2, min(30, max(2, len(train) // 2)))\n",
    "        smoothing_level = trial.suggest_float(\"smoothing_level\", 0.01, 1.0)\n",
    "        smoothing_slope = trial.suggest_float(\"smoothing_slope\", 0.01, 1.0)\n",
    "        smoothing_seasonal = trial.suggest_float(\"smoothing_seasonal\", 0.01, 1.0)\n",
    "\n",
    "        try:\n",
    "            model = ExponentialSmoothing(train_series, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_period)\n",
    "            fit_model = model.fit(smoothing_level=smoothing_level, smoothing_slope=smoothing_slope, smoothing_seasonal=smoothing_seasonal)\n",
    "            forecast = fit_model.forecast(len(test))\n",
    "            return smape(test, forecast)\n",
    "        except:\n",
    "            return float(\"inf\")\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20, timeout=60)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    best_model = ExponentialSmoothing(train_series, \n",
    "                                      trend=best_params[\"trend\"], \n",
    "                                      seasonal=best_params[\"seasonal\"], \n",
    "                                      seasonal_periods=best_params[\"seasonal_periods\"]).fit(\n",
    "                                        smoothing_level=best_params[\"smoothing_level\"],\n",
    "                                        smoothing_slope=best_params[\"smoothing_slope\"],\n",
    "                                        smoothing_seasonal=best_params[\"smoothing_seasonal\"]\n",
    "                                      )\n",
    "    return best_model.forecast(len(test))\n",
    "\n",
    "# --- ML Models (RandomForest, XGBoost, SVR, etc.) for Daily Data ---\n",
    "def run_ml_model(model, train, test, train_features, test_features):\n",
    "    model.fit(train_features, train)\n",
    "    base_predictions = model.predict(test_features)\n",
    "    base_smape = smape(test, base_predictions)\n",
    "\n",
    "    def objective(trial):\n",
    "        param_grid = {\n",
    "            \"RandomForestRegressor\": {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300, step=50),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            },\n",
    "            \"XGBRegressor\": {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300, step=50),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            },\n",
    "            \"LGBMRegressor\": {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300, step=50),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            },\n",
    "            \"CatBoostRegressor\": {\n",
    "                \"iterations\": trial.suggest_int(\"iterations\", 50, 300, step=50),\n",
    "                \"depth\": trial.suggest_int(\"depth\", 3, 10),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            },\n",
    "            \"SVR\": {\n",
    "                \"C\": trial.suggest_float(\"C\", 0.1, 10),\n",
    "                \"epsilon\": trial.suggest_float(\"epsilon\", 0.01, 1),\n",
    "                \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\", \"poly\"]),\n",
    "            },\n",
    "            \"KNeighborsRegressor\": {\n",
    "                \"n_neighbors\": trial.suggest_int(\"n_neighbors\", 2, 20),\n",
    "                \"weights\": trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        model_class = model.__class__.__name__\n",
    "        tuned_model = model.__class__(**param_grid.get(model_class, {}))\n",
    "        tuned_model.fit(train_features, train)\n",
    "        predictions = tuned_model.predict(test_features)\n",
    "        return smape(test, predictions)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20, timeout=120)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    tuned_model = model.__class__(**{k: v for k, v in best_params.items() if k in model.get_params()})\n",
    "    tuned_model.fit(train_features, train)\n",
    "    tuned_predictions = tuned_model.predict(test_features)\n",
    "    \n",
    "    return tuned_predictions if smape(test, tuned_predictions) < base_smape else base_predictions\n",
    "\n",
    "# Model for Prophet (Daily Adaptation)\n",
    "def run_prophet(train, test):\n",
    "    import pandas as pd\n",
    "    import optuna\n",
    "    from prophet import Prophet\n",
    "\n",
    "    # Copy and reset index\n",
    "    train_df = train.copy()\n",
    "    test_df = test.copy()\n",
    "\n",
    "    if train_df.index.name == \"Date\":\n",
    "        train_df.reset_index(inplace=True)\n",
    "    if test_df.index.name == \"Date\":\n",
    "        test_df.reset_index(inplace=True)\n",
    "\n",
    "    # Rename columns for Prophet\n",
    "    train_df.rename(columns={\"Date\": \"ds\", \"Target\": \"y\"}, inplace=True)\n",
    "    test_df.rename(columns={\"Date\": \"ds\"}, inplace=True)\n",
    "\n",
    "    # Ensure datetime format\n",
    "    train_df[\"ds\"] = pd.to_datetime(train_df[\"ds\"])\n",
    "    test_df[\"ds\"] = pd.to_datetime(test_df[\"ds\"])\n",
    "\n",
    "    # Check for holiday regressor\n",
    "    has_holiday = \"Holiday\" in train_df.columns\n",
    "    if has_holiday:\n",
    "        train_df[\"holiday\"] = train_df[\"Holiday\"].fillna(0)\n",
    "\n",
    "    def objective(trial):\n",
    "        # Hyperparameter space\n",
    "        params = {\n",
    "            \"changepoint_prior_scale\": trial.suggest_float(\"changepoint_prior_scale\", 0.001, 0.5, log=True),\n",
    "            \"seasonality_prior_scale\": trial.suggest_float(\"seasonality_prior_scale\", 0.01, 10.0, log=True),\n",
    "            \"seasonality_mode\": trial.suggest_categorical(\"seasonality_mode\", [\"additive\", \"multiplicative\"])\n",
    "        }\n",
    "\n",
    "        # Initialize Prophet model\n",
    "        model = Prophet(\n",
    "            changepoint_prior_scale=params[\"changepoint_prior_scale\"],\n",
    "            seasonality_prior_scale=params[\"seasonality_prior_scale\"],\n",
    "            seasonality_mode=params[\"seasonality_mode\"],\n",
    "            yearly_seasonality=True,\n",
    "            weekly_seasonality=True,\n",
    "            daily_seasonality=True\n",
    "        )\n",
    "\n",
    "        # Add regressors if applicable\n",
    "        if has_holiday:\n",
    "            model.add_regressor(\"holiday\")\n",
    "\n",
    "        # Fit model\n",
    "        model.fit(train_df)\n",
    "\n",
    "        # Create future dataframe\n",
    "        future = model.make_future_dataframe(periods=len(test_df), freq=\"D\")\n",
    "        if has_holiday:\n",
    "            holiday_df = train_df[[\"ds\", \"holiday\"]]\n",
    "            future = future.merge(holiday_df, on=\"ds\", how=\"left\").fillna(0)\n",
    "\n",
    "        # Generate forecast\n",
    "        forecast = model.predict(future)\n",
    "        y_pred = forecast.loc[forecast[\"ds\"].isin(test_df[\"ds\"]), \"yhat\"].values\n",
    "\n",
    "        # Handle misalignment\n",
    "        if len(y_pred) != len(test_df):\n",
    "            return float(\"inf\")\n",
    "\n",
    "        return smape(test[\"Target\"].values, y_pred)\n",
    "\n",
    "    # Optimize Prophet using Optuna\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20, show_progress_bar=False)\n",
    "\n",
    "    # Best model training\n",
    "    best_params = study.best_params\n",
    "    final_model = Prophet(\n",
    "        changepoint_prior_scale=best_params[\"changepoint_prior_scale\"],\n",
    "        seasonality_prior_scale=best_params[\"seasonality_prior_scale\"],\n",
    "        seasonality_mode=best_params[\"seasonality_mode\"],\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=True,\n",
    "        daily_seasonality=True\n",
    "    )\n",
    "\n",
    "    if has_holiday:\n",
    "        final_model.add_regressor(\"holiday\")\n",
    "\n",
    "    final_model.fit(train_df)\n",
    "\n",
    "    future = final_model.make_future_dataframe(periods=len(test_df), freq=\"D\")\n",
    "    if has_holiday:\n",
    "        holiday_df = train_df[[\"ds\", \"holiday\"]]\n",
    "        future = future.merge(holiday_df, on=\"ds\", how=\"left\").fillna(0)\n",
    "\n",
    "    forecast = final_model.predict(future)\n",
    "    return forecast.loc[forecast[\"ds\"].isin(test_df[\"ds\"]), \"yhat\"].values\n",
    "\n",
    "  # Extract only test period forecasts\n",
    "\n",
    "# Function to calculate SMAPE\n",
    "def smape(actual, predicted):\n",
    "    return 100 * np.mean(np.abs(predicted - actual) / ((np.abs(actual) + np.abs(predicted)) / 2))\n",
    "\n",
    "# Function to calculate MFB\n",
    "def mfb(actual, predicted):\n",
    "    return np.sum(predicted - actual) / np.sum(actual)\n",
    "\n",
    "# Function to evaluate models\n",
    "def evaluate_models(train, test, train_features, test_features):\n",
    "    models = {\n",
    "        \"ARIMA\": run_arima(train, test),\n",
    "        \"SARIMA\": run_sarima(train, test),\n",
    "        \"ETS\": run_ets(train, test),\n",
    "        \"Holt-Winters\": run_holt_winters(train, test),\n",
    "        \"Random Forest\": run_ml_model(RandomForestRegressor(n_estimators=100), train, test, train_features, test_features),\n",
    "        \"XGBoost\": run_ml_model(XGBRegressor(), train, test, train_features, test_features),\n",
    "        \"LightGBM\": run_ml_model(LGBMRegressor(), train, test, train_features, test_features),\n",
    "        \"CatBoost\": run_ml_model(CatBoostRegressor(verbose=0), train, test, train_features, test_features),\n",
    "        \"SVR\": run_ml_model(SVR(), train, test, train_features, test_features),\n",
    "        \"KNN\": run_ml_model(KNeighborsRegressor(), train, test, train_features, test_features),\n",
    "        \"LSTM\": run_lstm(train_features.values.reshape(-1, train_features.shape[1], 1),test_features.values.reshape(-1, test_features.shape[1], 1),train.values,test.values).flatten(),\n",
    "        \"GRU\":  run_gru(train_features.values.reshape(-1, train_features.shape[1], 1),test_features.values.reshape(-1, test_features.shape[1], 1),train.values,test.values).flatten(),\n",
    "        \"TFT\": run_tft(train_features, test_features, train, test),\n",
    "        \"Prophet\": run_prophet(train_features.join(train), test_features.join(test))\n",
    "    }\n",
    "    \n",
    "    smape_scores = {model: smape(test, pred) for model, pred in models.items()}\n",
    "    mfb_scores = {model: mfb(test, pred) for model, pred in models.items()}\n",
    "    \n",
    "    # Define thresholds\n",
    "    best_smape = min(smape_scores.values())\n",
    "    smape_threshold = 1.5 * best_smape\n",
    "    selected_models = {model: pred for model, pred in models.items() if smape_scores[model] <= smape_threshold and abs(mfb_scores[model]) <= 0.1}  # Keep models with MFB within ¬±10%\n",
    "    \n",
    "    print(\"\\n‚úÖ Models Used in Simple Average:\")\n",
    "    for model in selected_models.keys():\n",
    "        print(f\"- {model}\")\n",
    "    \n",
    "    # Compute Simple Average\n",
    "    simple_avg_forecast = np.mean(list(selected_models.values()), axis=0)\n",
    "    smape_simple_avg = smape(test, simple_avg_forecast)\n",
    "    mfb_simple_avg = mfb(test, simple_avg_forecast)\n",
    "    \n",
    "    # Compute Weighted Average\n",
    "    # Filter models that return array-like predictions (exclude dict-based like Prophet, SARIMA)\n",
    "    array_based_models = {\n",
    "        model: pred for model, pred in models.items()\n",
    "        if isinstance(pred, np.ndarray)\n",
    "    }\n",
    "    \n",
    "    # Recompute weights for array-based models only\n",
    "    inverse_smape = {\n",
    "        model: 1 / smape_scores[model] for model in array_based_models if smape_scores[model] > 0\n",
    "    }\n",
    "    total_weight = sum(inverse_smape.values())\n",
    "    weights = {model: weight / total_weight for model, weight in inverse_smape.items()}\n",
    "    \n",
    "    # Compute weighted average\n",
    "    weighted_avg_forecast = sum(weights[model] * array_based_models[model] for model in array_based_models)\n",
    "\n",
    "    smape_weighted_avg = smape(test, weighted_avg_forecast)\n",
    "    mfb_weighted_avg = mfb(test, weighted_avg_forecast)\n",
    "    \n",
    "    smape_scores[\"Simple Average\"] = smape_simple_avg\n",
    "    smape_scores[\"Weighted Average\"] = smape_weighted_avg\n",
    "    mfb_scores[\"Simple Average\"] = mfb_simple_avg\n",
    "    mfb_scores[\"Weighted Average\"] = mfb_weighted_avg\n",
    "\n",
    "    # Hybrid Model Selection\n",
    "    print(\"\\nüîÑ Evaluating Hybrid Models...\")\n",
    "    hybrid_model = None\n",
    "\n",
    "    if smape_scores[\"Weighted Average\"] < best_smape:\n",
    "        hybrid_model = \"Weighted Average\"\n",
    "    else:\n",
    "        best_stat_model = min([\"ARIMA\", \"SARIMA\", \"ETS\", \"Holt-Winters\"], key=lambda m: smape_scores.get(m, float(\"inf\")), default=None)\n",
    "        best_ml_model = min([\"Random Forest\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"SVR\", \"KNN\"], key=lambda m: smape_scores.get(m, float(\"inf\")), default=None)\n",
    "        best_dl_model = min([\"LSTM\", \"GRU\", \"TFT\"], key=lambda m: smape_scores.get(m, float(\"inf\")), default=None)\n",
    "    \n",
    "        if best_stat_model and best_ml_model:\n",
    "            hybrid_forecast = 0.5 * models[best_stat_model] + 0.5 * models[best_ml_model]\n",
    "            smape_hybrid = smape(test, hybrid_forecast)\n",
    "            print(f\"üìä Hybrid {best_stat_model} + {best_ml_model}: SMAPE = {smape_hybrid:.4f}\")\n",
    "            if smape_hybrid < best_smape:\n",
    "                hybrid_model = f\"{best_stat_model} + {best_ml_model}\"\n",
    "                models[hybrid_model] = hybrid_forecast\n",
    "                smape_scores[hybrid_model] = smape_hybrid\n",
    "    \n",
    "        elif best_ml_model and best_dl_model:\n",
    "            hybrid_forecast = 0.5 * models[best_ml_model] + 0.5 * models[best_dl_model]\n",
    "            smape_hybrid = smape(test, hybrid_forecast)\n",
    "            print(f\"üìä Hybrid {best_ml_model} + {best_dl_model}: SMAPE = {smape_hybrid:.4f}\")\n",
    "            if smape_hybrid < best_smape:\n",
    "                hybrid_model = f\"{best_ml_model} + {best_dl_model}\"\n",
    "                models[hybrid_model] = hybrid_forecast\n",
    "                smape_scores[hybrid_model] = smape_hybrid\n",
    "    \n",
    "    # Select Best Model\n",
    "    best_model = min(smape_scores, key=smape_scores.get)\n",
    "    \n",
    "    return models, smape_scores, mfb_scores, best_model, simple_avg_forecast, weighted_avg_forecast\n",
    "\n",
    "# Run pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "\n",
    "    # Run model evaluation\n",
    "    models, smape_scores, mfb_scores, best_model, simple_avg_forecast, weighted_avg_forecast = evaluate_models(\n",
    "        train[target_column], test[target_column], train_features, test_features\n",
    "    )\n",
    "\n",
    "    # Display SMAPE scores\n",
    "    print(\"\\nüîπ SMAPE Scores:\")\n",
    "    for model, score in smape_scores.items():\n",
    "        print(f\"{model}: {score:.4f}\")\n",
    "\n",
    "    # Display MFB scores\n",
    "    print(\"\\nüîπ MFB Scores:\")\n",
    "    for model, score in mfb_scores.items():\n",
    "        print(f\"{model}: {score:.4f}\")\n",
    "\n",
    "    # Show best model\n",
    "    print(f\"\\n‚úÖ Best Model: {best_model} (SMAPE: {smape_scores[best_model]:.4f}, MFB: {mfb_scores[best_model]:.4f})\")\n",
    "\n",
    "    # Model selection (only model name input, no forecast horizon)\n",
    "    while True:\n",
    "        selected_model = input(\"\\nEnter the model name you want to use for forecasting (from the above list): \").strip()\n",
    "        if selected_model in models or selected_model in [\"Simple Average\", \"Weighted Average\"]:\n",
    "            break\n",
    "        print(\"Invalid model selection. Please enter a valid model name from the list.\")\n",
    "\n",
    "    # Automatically use full test set length\n",
    "    forecast_horizon = len(test)\n",
    "\n",
    "    # Choose the forecast model\n",
    "    chosen_forecast = None\n",
    "    future_forecast_values = None\n",
    "\n",
    "    # Get forecast based on model selection\n",
    "    if selected_model == \"Simple Average\":\n",
    "        chosen_forecast = simple_avg_forecast\n",
    "        future_forecast_values = chosen_forecast[-forecast_horizon:]\n",
    "\n",
    "    elif selected_model == \"Weighted Average\":\n",
    "        chosen_forecast = weighted_avg_forecast\n",
    "        future_forecast_values = chosen_forecast[-forecast_horizon:]\n",
    "\n",
    "    elif selected_model in models:\n",
    "        model_output = models[selected_model]\n",
    "\n",
    "        # Handle Prophet\n",
    "        if selected_model == \"Prophet\":\n",
    "            chosen_forecast = model_output['yhat'][:len(test)]\n",
    "            future_forecast_values = model_output['yhat'][-forecast_horizon:]\n",
    "\n",
    "        # Handle SARIMA/ARIMA which returns separate components\n",
    "        elif isinstance(model_output, dict) and \"test\" in model_output and \"future\" in model_output:\n",
    "            chosen_forecast = model_output[\"test\"]\n",
    "            future_forecast_values = model_output[\"future\"]\n",
    "\n",
    "        # Generic forecast array\n",
    "        else:\n",
    "            chosen_forecast = model_output\n",
    "            future_forecast_values = chosen_forecast[-forecast_horizon:]\n",
    "\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Invalid model selection. Using best model.\")\n",
    "        chosen_forecast = models[best_model]\n",
    "        future_forecast_values = chosen_forecast[-forecast_horizon:]\n",
    "\n",
    "    # Format test index\n",
    "    test.index = pd.to_datetime(test.index)\n",
    "    formatted_test_index = test.index.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Compute residuals + confidence interval\n",
    "    residuals = test[target_column] - chosen_forecast\n",
    "    std_dev = np.std(residuals)\n",
    "    z_score = 1.96\n",
    "    lower_bound = chosen_forecast - (z_score * std_dev)\n",
    "    upper_bound = chosen_forecast + (z_score * std_dev)\n",
    "\n",
    "    # Forecast DataFrame for test\n",
    "    forecast_df = pd.DataFrame({\n",
    "        \"Date\": formatted_test_index,\n",
    "        \"Actual\": test[target_column].values,\n",
    "        \"Forecast\": chosen_forecast,\n",
    "        \"Lower Bound\": lower_bound,\n",
    "        \"Upper Bound\": upper_bound\n",
    "    })\n",
    "\n",
    "    print(\"\\nüìä Actual vs Forecast (Test Dataset) with Confidence Interval:\")\n",
    "    print(forecast_df.to_string(index=False))\n",
    "\n",
    "    # Generate future forecast index\n",
    "    last_test_date = test.index[-1]\n",
    "    future_dates = pd.date_range(start=last_test_date + pd.Timedelta(days=1), periods=forecast_horizon, freq='D')\n",
    "    formatted_future_dates = future_dates.strftime('%Y-%m-%d')\n",
    "\n",
    "    # Compute CI for future forecast\n",
    "    future_lower_bound = future_forecast_values - (z_score * std_dev)\n",
    "    future_upper_bound = future_forecast_values + (z_score * std_dev)\n",
    "\n",
    "    # Future forecast DataFrame\n",
    "    future_forecast_df = pd.DataFrame({\n",
    "        \"Date\": formatted_future_dates,\n",
    "        \"Forecast\": future_forecast_values,\n",
    "        \"Lower Bound\": future_lower_bound,\n",
    "        \"Upper Bound\": future_upper_bound\n",
    "    })\n",
    "\n",
    "    # Plot Actual vs Forecast with CI\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(formatted_test_index, test[target_column], label=\"Actual\", marker=\"o\", linestyle=\"-\", color=\"blue\")\n",
    "    plt.plot(formatted_test_index, chosen_forecast, label=f\"Forecast ({selected_model})\", linestyle=\"--\", color=\"red\")\n",
    "    plt.fill_between(formatted_test_index, lower_bound, upper_bound, color=\"red\", alpha=0.2, label=\"95% Confidence Interval\")\n",
    "\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Target\")\n",
    "    plt.title(\"Actual vs Forecasted Target with Confidence Interval\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print future forecast\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üß†  RP&KP | AutoForecast Suite ‚Äî Intelligent Time Series Predictions\".center(60))\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    print(f\"\\nüìÖ Future Forecast for the next {forecast_horizon} days using {selected_model}:\")\n",
    "    print(future_forecast_df.to_string(index=False))\n",
    "    print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2afdcbc-da36-4d61-9180-63b08c0183e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
