{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97c25b-628b-43fe-8f6d-b85ab9aa130b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Lasso\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_forecasting import TemporalFusionTransformer\n",
    "from pytorch_forecasting.data import TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import SMAPE\n",
    "import optuna\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from prophet import Prophet\n",
    "from scipy.stats import skew\n",
    "from itertools import combinations\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load dataset\n",
    "file_path = r\"D:\\Projects\\time_series\\Auto_Forecast\\data\\weekly_sales_5_years.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Try parsing weekly-compatible date formats\n",
    "parsed = False\n",
    "formats_to_try = [\"%Y-%m-%d\", \"%d-%m-%Y\", \"%Y/%m/%d\", \"%d/%m/%Y\"]\n",
    "\n",
    "for fmt in formats_to_try:\n",
    "    try:\n",
    "        df[\"Week\"] = pd.to_datetime(df[\"Week\"], format=fmt)\n",
    "        parsed = True\n",
    "        break\n",
    "    except ValueError:\n",
    "        continue\n",
    "\n",
    "if not parsed:\n",
    "    try:\n",
    "        df[\"Week\"] = pd.to_datetime(df[\"Week\"], infer_datetime_format=True)\n",
    "        parsed = True\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Unable to parse the 'Week' column. Please ensure it has consistent weekly date format.\")\n",
    "        raise e\n",
    "\n",
    "# Set the parsed 'Week' column as index\n",
    "df.set_index(\"Week\", inplace=True)\n",
    "print(\"‚úÖ Week column successfully parsed and set as index.\")\n",
    "\n",
    "# External features\n",
    "target_column = \"Sales\"\n",
    "external_features = [col for col in df.columns if col != target_column]\n",
    "df[external_features] = df[external_features].fillna(0)\n",
    "\n",
    "# Check data length (minimum 2 years = 104 weeks recommended)\n",
    "if len(df) < 104:\n",
    "    print(\"‚ùå Insufficient data. Please provide at least 2 years (104 weeks) of data.\")\n",
    "    sys.exit()\n",
    "\n",
    "# Handle missing values\n",
    "if df[target_column].isnull().sum() > 0:\n",
    "    print(\"‚ö†Ô∏è Missing values detected. Applying forward fill (ffill).\")\n",
    "    df[target_column].fillna(method='ffill', inplace=True)\n",
    "\n",
    "    if df[target_column].isnull().sum() > 0:\n",
    "        print(\"‚ö†Ô∏è Some values are still missing. Applying linear interpolation.\")\n",
    "        df[target_column].interpolate(method='linear', inplace=True)\n",
    "\n",
    "    print(\"‚ö†Ô∏è Forecast accuracy may be affected due to Existence of Missing Values.\")\n",
    "\n",
    "# Check skewness\n",
    "from scipy.stats import skew\n",
    "skewness = skew(df[target_column])\n",
    "print(f\"Skewness: {skewness}\")\n",
    "\n",
    "if abs(skewness) > 1:\n",
    "    print(\"‚ö†Ô∏è Skewness is high, applying log transformation.\")\n",
    "    df[target_column] = np.log(df[target_column] + 1)\n",
    "else:\n",
    "    print(\"‚úÖ Skewness is acceptable. No transformation applied.\")\n",
    "\n",
    "# Detect & treat outliers using IQR\n",
    "Q1, Q3 = df[target_column].quantile([0.25, 0.75])\n",
    "IQR = Q3 - Q1\n",
    "lower_bound, upper_bound = Q1 - 1.5 * IQR, Q3 + 1.5 * IQR\n",
    "outliers = df[(df[target_column] < lower_bound) | (df[target_column] > upper_bound)]\n",
    "\n",
    "if not outliers.empty:\n",
    "    print(\"‚ö†Ô∏è Outliers detected. Replacing with median.\")\n",
    "    df.loc[df[target_column] < lower_bound, target_column] = df[target_column].median()\n",
    "    df.loc[df[target_column] > upper_bound, target_column] = df[target_column].median()\n",
    "\n",
    "# Generate lag features (up to 52 weeks)\n",
    "for lag in range(1, 53):\n",
    "    df[f'lag_{lag}'] = df[target_column].shift(lag)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# ACF/PACF selection\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "max_lags = min(52, len(df) // 2 - 1)\n",
    "acf_values = acf(df[target_column], nlags=max_lags)[1:]\n",
    "pacf_values = pacf(df[target_column], nlags=max_lags)[1:]\n",
    "selected_lags = [i + 1 for i, (a, p) in enumerate(zip(acf_values, pacf_values)) if abs(a) > 0.2 or abs(p) > 0.2]\n",
    "\n",
    "if not selected_lags:\n",
    "    selected_lags = list(range(1, 53))\n",
    "\n",
    "# VIF filtering\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "selected_features = [f'lag_{lag}' for lag in selected_lags]\n",
    "while len(selected_features) > 1:\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data['Feature'] = selected_features\n",
    "    vif_data['VIF'] = [variance_inflation_factor(df[selected_features].values, i) for i in range(len(selected_features))]\n",
    "\n",
    "    max_vif = vif_data['VIF'].max()\n",
    "    if max_vif > 5:\n",
    "        selected_features.remove(vif_data.loc[vif_data['VIF'].idxmax(), 'Feature'])\n",
    "    else:\n",
    "        break\n",
    "\n",
    "# RMSE-based lag selection\n",
    "from itertools import combinations\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def select_best_lags(train_features, test_features, train_target, test_target, model, max_lags=5):\n",
    "    best_rmse, best_lags = float(\"inf\"), None\n",
    "    all_lags = [col for col in train_features.columns if col.startswith(\"lag_\")]\n",
    "\n",
    "    for r in range(1, min(len(all_lags), max_lags) + 1):\n",
    "        for lag_subset in combinations(all_lags, r):\n",
    "            try:\n",
    "                train_subset = train_features[list(lag_subset)]\n",
    "                test_subset = test_features[list(lag_subset)]\n",
    "                model.fit(train_subset, train_target)\n",
    "                predictions = model.predict(test_subset)\n",
    "                rmse = np.sqrt(mean_squared_error(test_target, predictions))\n",
    "                if rmse < best_rmse:\n",
    "                    best_rmse, best_lags = rmse, list(lag_subset)\n",
    "            except Exception:\n",
    "                continue\n",
    "    return best_lags if best_lags else all_lags\n",
    "\n",
    "# Train/test split\n",
    "split_index = int(len(df) * 0.8)\n",
    "train_df, test_df = df.iloc[:split_index], df.iloc[split_index:]\n",
    "train_features = train_df[selected_features]\n",
    "test_features = test_df[selected_features]\n",
    "train_target = train_df[target_column]\n",
    "test_target = test_df[target_column]\n",
    "\n",
    "model = LinearRegression()\n",
    "best_lags = select_best_lags(train_features, test_features, train_target, test_target, model)\n",
    "selected_features = best_lags\n",
    "\n",
    "# Final Lasso Filtering\n",
    "from sklearn.linear_model import Lasso\n",
    "X = df[selected_features]\n",
    "y = df[target_column]\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X, y)\n",
    "selected_features = [feature for feature, coef in zip(selected_features, lasso.coef_) if coef != 0]\n",
    "\n",
    "# ‚úÖ Apply rolling statistics dynamically (optimized for RMSE)\n",
    "best_window = 3  # Start with 3 weeks\n",
    "best_rmse = float(\"inf\")\n",
    "\n",
    "train_size = int(len(df) * 0.8)\n",
    "train, test = df.iloc[:train_size], df.iloc[train_size:]\n",
    "\n",
    "for window in range(3, 37):\n",
    "    rolling_mean = train[target_column].rolling(window=window, min_periods=1).mean()\n",
    "    rmse = np.sqrt(mean_squared_error(test[target_column], rolling_mean[-len(test):]))\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_window = window\n",
    "\n",
    "print(f\"‚úÖ Best rolling window (weekly): {best_window}\")\n",
    "df[target_column] = df[target_column].rolling(window=best_window, min_periods=1).mean()\n",
    "\n",
    "# ‚úÖ Recreate train-test split post rolling\n",
    "train_size = int(len(df) * 0.8)\n",
    "train, test = df.iloc[:train_size], df.iloc[train_size:]\n",
    "train_features, test_features = train[selected_features], test[selected_features]\n",
    "train_target, test_target = train[target_column], test[target_column]\n",
    "\n",
    "# ‚úÖ Evaluation metrics for all models\n",
    "def smape(actual, forecast):\n",
    "    return 100 * np.mean(2 * np.abs(forecast - actual) / (np.abs(actual) + np.abs(forecast)))\n",
    "\n",
    "def mean_forecast_bias(actual, forecast):\n",
    "    return np.mean(forecast - actual)\n",
    "\n",
    "def evaluate_model(actual, forecast):\n",
    "    return smape(actual, forecast), mean_forecast_bias(actual, forecast)\n",
    "\n",
    "# ‚úÖ TFT-like Dense Model with Optuna tuning\n",
    "def run_tft(train_features, test_features, train_target, test_target):\n",
    "    def objective(trial):\n",
    "        units = trial.suggest_int('units', 32, 256)\n",
    "        dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "\n",
    "        model = Sequential([\n",
    "            Dense(units, activation='relu', input_shape=(train_features.shape[1],)),\n",
    "            Dropout(dropout),\n",
    "            Dense(units // 2, activation='relu'),\n",
    "            Dropout(dropout),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='mse')\n",
    "        model.fit(train_features, train_target, epochs=50, batch_size=16, verbose=0)\n",
    "        preds = model.predict(test_features).flatten()\n",
    "        return smape(test_target, preds)\n",
    "\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    model = Sequential([\n",
    "        Dense(best_params['units'], activation='relu', input_shape=(train_features.shape[1],)),\n",
    "        Dropout(best_params['dropout']),\n",
    "        Dense(best_params['units'] // 2, activation='relu'),\n",
    "        Dropout(best_params['dropout']),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.fit(train_features, train_target, epochs=50, batch_size=16, verbose=0)\n",
    "    optimized_preds = model.predict(test_features).flatten()\n",
    "\n",
    "    baseline_model = Sequential([\n",
    "        Dense(128, activation='relu', input_shape=(train_features.shape[1],)),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    baseline_model.compile(optimizer='adam', loss='mse')\n",
    "    baseline_model.fit(train_features, train_target, epochs=50, batch_size=16, verbose=0)\n",
    "    baseline_preds = baseline_model.predict(test_features).flatten()\n",
    "\n",
    "    smape_base, mfb_base = evaluate_model(test_target, baseline_preds)\n",
    "    smape_opt, mfb_opt = evaluate_model(test_target, optimized_preds)\n",
    "\n",
    "    return optimized_preds if smape_opt < smape_base and abs(mfb_opt) < abs(mfb_base) else baseline_preds\n",
    "\n",
    "# ‚úÖ LSTM Model with Optuna\n",
    "def run_lstm(train_features, test_features, train_target, test_target):\n",
    "    train_X = np.asarray(train_features).reshape(train_features.shape[0], train_features.shape[1], 1)\n",
    "    test_X = np.asarray(test_features).reshape(test_features.shape[0], test_features.shape[1], 1)\n",
    "\n",
    "    train_y = np.asarray(train_target).reshape(-1, 1)\n",
    "    test_y = np.asarray(test_target).reshape(-1, 1)\n",
    "\n",
    "    def objective(trial):\n",
    "        n_units = trial.suggest_int(\"n_units\", 16, 128)\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "        learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "\n",
    "        model = Sequential([\n",
    "            LSTM(n_units, activation='relu', return_sequences=True, input_shape=(train_X.shape[1], 1)),\n",
    "            Dropout(dropout_rate),\n",
    "            LSTM(n_units, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "        model.fit(train_X, train_y, epochs=50, batch_size=16, verbose=0)\n",
    "        predictions = model.predict(test_X).flatten()\n",
    "        return smape(test_y, predictions)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_model = Sequential([\n",
    "        LSTM(best_trial.params[\"n_units\"], activation='relu', return_sequences=True, input_shape=(train_X.shape[1], 1)),\n",
    "        Dropout(best_trial.params[\"dropout_rate\"]),\n",
    "        LSTM(best_trial.params[\"n_units\"], activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    best_model.compile(optimizer=Adam(learning_rate=best_trial.params[\"learning_rate\"]), loss='mse')\n",
    "    best_model.fit(train_X, train_y, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "    final_predictions = best_model.predict(test_X).flatten()\n",
    "    return final_predictions\n",
    "\n",
    "# ‚úÖ GRU Model with Optuna and dynamic baseline comparison\n",
    "def run_gru(train_features, test_features, train_target, test_target):\n",
    "    train_X = np.asarray(train_features).reshape(train_features.shape[0], train_features.shape[1], 1)\n",
    "    test_X = np.asarray(test_features).reshape(test_features.shape[0], test_features.shape[1], 1)\n",
    "\n",
    "    train_y = np.asarray(train_target).reshape(-1, 1)\n",
    "    test_y = np.asarray(test_target).reshape(-1, 1)\n",
    "\n",
    "    def objective(trial):\n",
    "        n_units = trial.suggest_int(\"n_units\", 16, 128)\n",
    "        dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "        learning_rate = trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2)\n",
    "\n",
    "        model = Sequential([\n",
    "            GRU(n_units, activation='relu', return_sequences=True, input_shape=(train_X.shape[1], 1)),\n",
    "            Dropout(dropout_rate),\n",
    "            GRU(n_units, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')\n",
    "        model.fit(train_X, train_y, epochs=50, batch_size=16, verbose=0)\n",
    "        predictions = model.predict(test_X).flatten()\n",
    "        return smape(test_y, predictions)\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    best_trial = study.best_trial\n",
    "    best_model = Sequential([\n",
    "        GRU(best_trial.params[\"n_units\"], activation='relu', return_sequences=True, input_shape=(train_X.shape[1], 1)),\n",
    "        Dropout(best_trial.params[\"dropout_rate\"]),\n",
    "        GRU(best_trial.params[\"n_units\"], activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    best_model.compile(optimizer=Adam(learning_rate=best_trial.params[\"learning_rate\"]), loss='mse')\n",
    "    best_model.fit(train_X, train_y, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "    baseline_model = Sequential([\n",
    "        GRU(50, activation='relu', return_sequences=True, input_shape=(train_X.shape[1], 1)),\n",
    "        GRU(50, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    baseline_model.compile(optimizer='adam', loss='mse')\n",
    "    baseline_model.fit(train_X, train_y, epochs=50, batch_size=16, verbose=0)\n",
    "\n",
    "    best_preds = best_model.predict(test_X).flatten()\n",
    "    baseline_preds = baseline_model.predict(test_X).flatten()\n",
    "\n",
    "    if smape(test_y, best_preds) < smape(test_y, baseline_preds) and abs(mean_forecast_bias(test_y, best_preds)) < abs(mean_forecast_bias(test_y, baseline_preds)):\n",
    "        return best_preds\n",
    "    else:\n",
    "        return baseline_preds\n",
    "\n",
    "# Determine frequency-based seasonality\n",
    "def get_seasonal_period(data):\n",
    "    inferred_freq = pd.infer_freq(data.index)\n",
    "    if inferred_freq in ['W', 'W-MON', 'W-SUN']:\n",
    "        return 52  # Weekly data\n",
    "    elif inferred_freq in ['MS', 'M']:\n",
    "        return 12  # Monthly data\n",
    "    else:\n",
    "        return max(2, min(12, len(data) // 2))  # Fallback\n",
    "\n",
    "# Function to calculate SMAPE\n",
    "def smape(y_true, y_pred):\n",
    "    return 100 * np.mean(2 * np.abs(y_pred - y_true) / (np.abs(y_true) + np.abs(y_pred)))\n",
    "\n",
    "# Function to calculate MFB\n",
    "def mfb(y_true, y_pred):\n",
    "    return 100 * np.mean((y_pred - y_true) / y_true)\n",
    "\n",
    "# ARIMA\n",
    "def run_arima(train, test):\n",
    "    best_loss, best_order = float(\"inf\"), None\n",
    "    lambda_mfb = 0.5\n",
    "\n",
    "    for p in range(3):\n",
    "        for d in range(2):\n",
    "            for q in range(3):\n",
    "                try:\n",
    "                    model = ARIMA(train, order=(p, d, q)).fit()\n",
    "                    pred = model.forecast(steps=len(test))\n",
    "                    smape_score = smape(test, pred)\n",
    "                    mfb_score = abs(mfb(test, pred))\n",
    "                    loss = smape_score + lambda_mfb * mfb_score\n",
    "                    if loss < best_loss:\n",
    "                        best_loss, best_order = loss, (p, d, q)\n",
    "                except:\n",
    "                    continue\n",
    "    final_model = ARIMA(train, order=best_order).fit()\n",
    "    return final_model.forecast(steps=len(test))\n",
    "\n",
    "# SARIMA\n",
    "def run_sarima(train, test):\n",
    "    best_score, best_order, best_seasonal_order = float(\"inf\"), None, None\n",
    "    seasonal_period = get_seasonal_period(train)\n",
    "\n",
    "    for p in range(3):\n",
    "        for d in range(2):\n",
    "            for q in range(3):\n",
    "                for P in range(2):\n",
    "                    for D in range(2):\n",
    "                        for Q in range(2):\n",
    "                            try:\n",
    "                                model = SARIMAX(train, order=(p, d, q), seasonal_order=(P, D, Q, seasonal_period)).fit()\n",
    "                                pred = model.forecast(steps=len(test))\n",
    "                                score = smape(test, pred) + abs(mfb(test, pred))\n",
    "                                if score < best_score:\n",
    "                                    best_score, best_order, best_seasonal_order = score, (p, d, q), (P, D, Q, seasonal_period)\n",
    "                            except:\n",
    "                                continue\n",
    "    return SARIMAX(train, order=best_order, seasonal_order=best_seasonal_order).fit().forecast(steps=len(test))\n",
    "\n",
    "# ETS\n",
    "def run_ets(train, test):\n",
    "    seasonal_period = get_seasonal_period(train)\n",
    "\n",
    "    def objective(trial):\n",
    "        trend = trial.suggest_categorical(\"trend\", [\"add\", \"mul\", None])\n",
    "        seasonal = trial.suggest_categorical(\"seasonal\", [\"add\", \"mul\", None])\n",
    "        seasonal_periods = trial.suggest_int(\"seasonal_periods\", 2, min(52, len(train) // 2))\n",
    "        try:\n",
    "            model = ExponentialSmoothing(train, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods).fit()\n",
    "            return smape(test, model.forecast(len(test)))\n",
    "        except:\n",
    "            return float(\"inf\")\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    best = study.best_params\n",
    "    try:\n",
    "        best_model = ExponentialSmoothing(train, trend=best[\"trend\"], seasonal=best[\"seasonal\"], seasonal_periods=best[\"seasonal_periods\"]).fit()\n",
    "    except:\n",
    "        best_model = ExponentialSmoothing(train, trend=\"add\", seasonal=\"add\", seasonal_periods=seasonal_period).fit()\n",
    "\n",
    "    return best_model.forecast(len(test))\n",
    "\n",
    "# Holt-Winters\n",
    "def run_holt_winters(train, test):\n",
    "    train_series = pd.Series(train)\n",
    "    seasonal_period = get_seasonal_period(train)\n",
    "\n",
    "    def objective(trial):\n",
    "        trend = trial.suggest_categorical(\"trend\", [\"add\", \"mul\", None])\n",
    "        seasonal = trial.suggest_categorical(\"seasonal\", [\"add\", \"mul\", None])\n",
    "        seasonal_periods = trial.suggest_int(\"seasonal_periods\", 2, min(52, len(train) // 2))\n",
    "        sl = trial.suggest_float(\"smoothing_level\", 0.01, 1.0)\n",
    "        ss = trial.suggest_float(\"smoothing_slope\", 0.01, 1.0)\n",
    "        seas = trial.suggest_float(\"smoothing_seasonal\", 0.01, 1.0)\n",
    "\n",
    "        try:\n",
    "            model = ExponentialSmoothing(train_series, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods)\n",
    "            fitted = model.fit(smoothing_level=sl, smoothing_slope=ss, smoothing_seasonal=seas)\n",
    "            return smape(test, fitted.forecast(len(test)))\n",
    "        except:\n",
    "            return float(\"inf\")\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20, timeout=60)\n",
    "\n",
    "    best = study.best_params\n",
    "    model = ExponentialSmoothing(train_series, trend=best[\"trend\"], seasonal=best[\"seasonal\"], seasonal_periods=best[\"seasonal_periods\"])\n",
    "    fitted = model.fit(smoothing_level=best[\"smoothing_level\"], smoothing_slope=best[\"smoothing_slope\"], smoothing_seasonal=best[\"smoothing_seasonal\"])\n",
    "    return fitted.forecast(len(test))\n",
    "\n",
    "# ML models\n",
    "def run_ml_model(model, train, test, train_features, test_features):\n",
    "    model.fit(train_features, train)\n",
    "    base_pred = model.predict(test_features)\n",
    "    base_score = smape(test, base_pred)\n",
    "\n",
    "    def objective(trial):\n",
    "        param_grid = {\n",
    "            \"RandomForestRegressor\": {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300, step=50),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n",
    "            },\n",
    "            \"XGBRegressor\": {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300, step=50),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            },\n",
    "            \"LGBMRegressor\": {\n",
    "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300, step=50),\n",
    "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 20),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            },\n",
    "            \"CatBoostRegressor\": {\n",
    "                \"iterations\": trial.suggest_int(\"iterations\", 50, 300, step=50),\n",
    "                \"depth\": trial.suggest_int(\"depth\", 3, 10),\n",
    "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.3),\n",
    "            },\n",
    "            \"SVR\": {\n",
    "                \"C\": trial.suggest_float(\"C\", 0.1, 10),\n",
    "                \"epsilon\": trial.suggest_float(\"epsilon\", 0.01, 1),\n",
    "                \"kernel\": trial.suggest_categorical(\"kernel\", [\"linear\", \"rbf\", \"poly\"]),\n",
    "            },\n",
    "            \"KNeighborsRegressor\": {\n",
    "                \"n_neighbors\": trial.suggest_int(\"n_neighbors\", 2, 20),\n",
    "                \"weights\": trial.suggest_categorical(\"weights\", [\"uniform\", \"distance\"]),\n",
    "            },\n",
    "        }\n",
    "\n",
    "        model_class = model.__class__.__name__\n",
    "        tuned = model.__class__(**param_grid.get(model_class, {}))\n",
    "        tuned.fit(train_features, train)\n",
    "        return smape(test, tuned.predict(test_features))\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=20, timeout=120)\n",
    "\n",
    "    best_params = study.best_params\n",
    "    tuned_model = model.__class__(**{k: v for k, v in best_params.items() if k in model.get_params()})\n",
    "    tuned_model.fit(train_features, train)\n",
    "    tuned_pred = tuned_model.predict(test_features)\n",
    "\n",
    "    return tuned_pred if smape(test, tuned_pred) < base_score else base_pred\n",
    "\n",
    "# Prophet\n",
    "def run_prophet(train, test):\n",
    "    # If train/test are Series, convert to Prophet-compatible DataFrame\n",
    "    if isinstance(train, pd.Series):\n",
    "        df_train = pd.DataFrame({'ds': train.index, 'y': train.values})\n",
    "    else:\n",
    "        df_train = train.rename(columns={train.columns[0]: 'ds', train.columns[1]: 'y'})\n",
    "\n",
    "    if isinstance(test, pd.Series):\n",
    "        df_test = pd.DataFrame({'ds': test.index, 'y': test.values})\n",
    "    else:\n",
    "        df_test = test.rename(columns={test.columns[0]: 'ds', test.columns[1]: 'y'})\n",
    "\n",
    "    model = Prophet()\n",
    "\n",
    "    # Optional: Add regressors if present\n",
    "    if not isinstance(train, pd.Series) and 'holiday' in train.columns:\n",
    "        df_train['holiday'] = train['holiday'].values\n",
    "        model.add_regressor('holiday')\n",
    "\n",
    "    model.fit(df_train)\n",
    "\n",
    "    # Infer frequency or fall back to weekly\n",
    "    freq = pd.infer_freq(df_train['ds']) or \"W\"\n",
    "    future = model.make_future_dataframe(periods=len(df_test), freq=freq)\n",
    "\n",
    "    # Add regressors to future if used\n",
    "    if not isinstance(test, pd.Series) and 'holiday' in test.columns:\n",
    "        future['holiday'] = test['holiday'].values\n",
    "\n",
    "    forecast = model.predict(future)\n",
    "\n",
    "    return forecast[['yhat']].iloc[-len(df_test):].values.flatten()\n",
    "\n",
    "# SMAPE\n",
    "def smape(actual, predicted):\n",
    "    return 100 * np.mean(np.abs(predicted - actual) / ((np.abs(actual) + np.abs(predicted)) / 2))\n",
    "\n",
    "# MFB\n",
    "def mfb(actual, predicted):\n",
    "    return np.sum(predicted - actual) / np.sum(actual)\n",
    "\n",
    "# Model Evaluation Pipeline\n",
    "def evaluate_models(train, test, train_features, test_features):\n",
    "    models = {\n",
    "        \"ARIMA\": run_arima(train, test),\n",
    "        \"SARIMA\": run_sarima(train, test),\n",
    "        \"ETS\": run_ets(train, test),\n",
    "        \"Holt-Winters\": run_holt_winters(train, test),\n",
    "        \"Random Forest\": run_ml_model(RandomForestRegressor(n_estimators=100), train, test, train_features, test_features),\n",
    "        \"XGBoost\": run_ml_model(XGBRegressor(), train, test, train_features, test_features),\n",
    "        \"LightGBM\": run_ml_model(LGBMRegressor(), train, test, train_features, test_features),\n",
    "        \"CatBoost\": run_ml_model(CatBoostRegressor(verbose=0), train, test, train_features, test_features),\n",
    "        \"SVR\": run_ml_model(SVR(), train, test, train_features, test_features),\n",
    "        \"KNN\": run_ml_model(KNeighborsRegressor(), train, test, train_features, test_features),\n",
    "        \"LSTM\": run_lstm(train_features.values.reshape(-1, train_features.shape[1], 1),\n",
    "                         test_features.values.reshape(-1, test_features.shape[1], 1),\n",
    "                         train.values, test.values).flatten(),\n",
    "        \"GRU\":  run_gru(train_features.values.reshape(-1, train_features.shape[1], 1),\n",
    "                        test_features.values.reshape(-1, test_features.shape[1], 1),\n",
    "                        train.values, test.values).flatten(),\n",
    "        \"TFT\": run_tft(train_features, test_features, train, test),\n",
    "        \"Prophet\": run_prophet(train, test)\n",
    "    }\n",
    "\n",
    "    smape_scores = {model: smape(test, pred) for model, pred in models.items()}\n",
    "    mfb_scores = {model: mfb(test, pred) for model, pred in models.items()}\n",
    "\n",
    "    best_smape = min(smape_scores.values())\n",
    "    smape_threshold = 1.5 * best_smape\n",
    "    selected_models = {\n",
    "        model: pred for model, pred in models.items()\n",
    "        if smape_scores[model] <= smape_threshold and abs(mfb_scores[model]) <= 0.1\n",
    "    }\n",
    "\n",
    "    print(\"\\n‚úÖ Models Used in Simple Average:\")\n",
    "    for model in selected_models:\n",
    "        print(f\"- {model}\")\n",
    "\n",
    "    simple_avg_forecast = np.mean(list(selected_models.values()), axis=0)\n",
    "    smape_simple_avg = smape(test, simple_avg_forecast)\n",
    "    mfb_simple_avg = mfb(test, simple_avg_forecast)\n",
    "\n",
    "    array_based_models = {\n",
    "        model: pred for model, pred in models.items()\n",
    "        if isinstance(pred, np.ndarray)\n",
    "    }\n",
    "    inverse_smape = {\n",
    "        model: 1 / smape_scores[model] for model in array_based_models if smape_scores[model] > 0\n",
    "    }\n",
    "    total_weight = sum(inverse_smape.values())\n",
    "    weights = {model: weight / total_weight for model, weight in inverse_smape.items()}\n",
    "\n",
    "    weighted_avg_forecast = sum(weights[model] * array_based_models[model] for model in array_based_models)\n",
    "    smape_weighted_avg = smape(test, weighted_avg_forecast)\n",
    "    mfb_weighted_avg = mfb(test, weighted_avg_forecast)\n",
    "\n",
    "    smape_scores[\"Simple Average\"] = smape_simple_avg\n",
    "    smape_scores[\"Weighted Average\"] = smape_weighted_avg\n",
    "    mfb_scores[\"Simple Average\"] = mfb_simple_avg\n",
    "    mfb_scores[\"Weighted Average\"] = mfb_weighted_avg\n",
    "\n",
    "    print(\"\\nüîÑ Evaluating Hybrid Models...\")\n",
    "    hybrid_model = None\n",
    "\n",
    "    if smape_scores[\"Weighted Average\"] < best_smape:\n",
    "        hybrid_model = \"Weighted Average\"\n",
    "    else:\n",
    "        best_stat_model = min([\"ARIMA\", \"SARIMA\", \"ETS\", \"Holt-Winters\"], key=lambda m: smape_scores.get(m, float(\"inf\")))\n",
    "        best_ml_model = min([\"Random Forest\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"SVR\", \"KNN\"], key=lambda m: smape_scores.get(m, float(\"inf\")))\n",
    "        best_dl_model = min([\"LSTM\", \"GRU\", \"TFT\"], key=lambda m: smape_scores.get(m, float(\"inf\")))\n",
    "\n",
    "        if best_stat_model and best_ml_model:\n",
    "            hybrid_forecast = 0.5 * models[best_stat_model] + 0.5 * models[best_ml_model]\n",
    "            smape_hybrid = smape(test, hybrid_forecast)\n",
    "            print(f\"üìä Hybrid {best_stat_model} + {best_ml_model}: SMAPE = {smape_hybrid:.4f}\")\n",
    "            if smape_hybrid < best_smape:\n",
    "                hybrid_model = f\"{best_stat_model} + {best_ml_model}\"\n",
    "                models[hybrid_model] = hybrid_forecast\n",
    "                smape_scores[hybrid_model] = smape_hybrid\n",
    "\n",
    "        elif best_ml_model and best_dl_model:\n",
    "            hybrid_forecast = 0.5 * models[best_ml_model] + 0.5 * models[best_dl_model]\n",
    "            smape_hybrid = smape(test, hybrid_forecast)\n",
    "            print(f\"üìä Hybrid {best_ml_model} + {best_dl_model}: SMAPE = {smape_hybrid:.4f}\")\n",
    "            if smape_hybrid < best_smape:\n",
    "                hybrid_model = f\"{best_ml_model} + {best_dl_model}\"\n",
    "                models[hybrid_model] = hybrid_forecast\n",
    "                smape_scores[hybrid_model] = smape_hybrid\n",
    "\n",
    "    best_model = min(smape_scores, key=smape_scores.get)\n",
    "\n",
    "    return models, smape_scores, mfb_scores, best_model, simple_avg_forecast, weighted_avg_forecast\n",
    "\n",
    "# --------------------------\n",
    "# Run Pipeline\n",
    "# --------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    models, smape_scores, mfb_scores, best_model, simple_avg_forecast, weighted_avg_forecast = evaluate_models(\n",
    "        train[target_column], test[target_column], train_features, test_features\n",
    "    )\n",
    "\n",
    "    print(\"\\nüîπ SMAPE Scores:\")\n",
    "    for model, score in smape_scores.items():\n",
    "        print(f\"{model}: {score:.4f}\")\n",
    "\n",
    "    print(\"\\nüîπ MFB Scores:\")\n",
    "    for model, score in mfb_scores.items():\n",
    "        print(f\"{model}: {score:.4f}\")\n",
    "\n",
    "    print(f\"\\n‚úÖ Best Model: {best_model} (SMAPE: {smape_scores[best_model]:.4f}, MFB: {mfb_scores[best_model]:.4f})\")\n",
    "\n",
    "    while True:\n",
    "        selected_model = input(\"\\nEnter the model name you want to use for forecasting (from the above list): \").strip()\n",
    "        if selected_model in models or selected_model in [\"Simple Average\", \"Weighted Average\"]:\n",
    "            break\n",
    "        print(\"Invalid model selection. Please enter a valid model name from the list.\")\n",
    "\n",
    "    forecast_horizon = len(test)\n",
    "    chosen_forecast = None\n",
    "    future_forecast_values = None\n",
    "\n",
    "    if selected_model == \"Simple Average\":\n",
    "        chosen_forecast = simple_avg_forecast\n",
    "        future_forecast_values = chosen_forecast[-forecast_horizon:]\n",
    "\n",
    "    elif selected_model == \"Weighted Average\":\n",
    "        chosen_forecast = weighted_avg_forecast\n",
    "        future_forecast_values = chosen_forecast[-forecast_horizon:]\n",
    "\n",
    "    elif selected_model in models:\n",
    "        model_output = models[selected_model]\n",
    "        if selected_model == \"Prophet\":\n",
    "            chosen_forecast = model_output['yhat'][:len(test)]\n",
    "            future_forecast_values = model_output['yhat'][-forecast_horizon:]\n",
    "        elif isinstance(model_output, dict) and \"test\" in model_output and \"future\" in model_output:\n",
    "            chosen_forecast = model_output[\"test\"]\n",
    "            future_forecast_values = model_output[\"future\"]\n",
    "        else:\n",
    "            chosen_forecast = model_output\n",
    "            future_forecast_values = chosen_forecast[-forecast_horizon:]\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Invalid model selection. Using best model.\")\n",
    "        chosen_forecast = models[best_model]\n",
    "        future_forecast_values = chosen_forecast[-forecast_horizon:]\n",
    "\n",
    "    test.index = pd.to_datetime(test.index)\n",
    "    formatted_test_index = test.index.strftime('%m-%Y')\n",
    "\n",
    "    residuals = test[target_column] - chosen_forecast\n",
    "    std_dev = np.std(residuals)\n",
    "    z_score = 1.96\n",
    "    lower_bound = chosen_forecast - (z_score * std_dev)\n",
    "    upper_bound = chosen_forecast + (z_score * std_dev)\n",
    "\n",
    "    forecast_df = pd.DataFrame({\n",
    "        \"Date\": formatted_test_index,\n",
    "        \"Actual\": test[target_column].values,\n",
    "        \"Forecast\": chosen_forecast,\n",
    "        \"Lower Bound\": lower_bound,\n",
    "        \"Upper Bound\": upper_bound\n",
    "    })\n",
    "\n",
    "    print(\"\\nüìä Actual vs Forecast (Test Dataset) with Confidence Interval:\")\n",
    "    print(forecast_df.to_string(index=False))\n",
    "\n",
    "    last_test_date = test.index[-1]\n",
    "    future_dates = pd.date_range(start=last_test_date, periods=forecast_horizon + 1, freq='W-MON')[1:]\n",
    "    formatted_future_dates = future_dates.strftime('%m-%Y')\n",
    "\n",
    "    future_lower_bound = future_forecast_values - (z_score * std_dev)\n",
    "    future_upper_bound = future_forecast_values + (z_score * std_dev)\n",
    "\n",
    "    future_forecast_df = pd.DataFrame({\n",
    "        \"Date\": formatted_future_dates,\n",
    "        \"Forecast\": future_forecast_values,\n",
    "        \"Lower Bound\": future_lower_bound,\n",
    "        \"Upper Bound\": future_upper_bound\n",
    "    })\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(formatted_test_index, test[target_column], label=\"Actual\", marker=\"o\", linestyle=\"-\", color=\"blue\")\n",
    "    plt.plot(formatted_test_index, chosen_forecast, label=f\"Forecast ({selected_model})\", linestyle=\"--\", color=\"red\")\n",
    "    plt.fill_between(formatted_test_index, lower_bound, upper_bound, color=\"red\", alpha=0.2, label=\"95% Confidence Interval\")\n",
    "\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Sales\")\n",
    "    plt.title(\"Actual vs Forecasted Sales with Confidence Interval\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"üß†  RP&KP | AutoForecast Suite ‚Äî Intelligent Time Series Predictions\".center(60))\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìÖ Future Forecast for the next {forecast_horizon} periods using {selected_model}:\")\n",
    "    print(future_forecast_df.to_string(index=False))\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b8d8fb-783f-4366-a8bf-78d3530e1efa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
